
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

RDDs are the lowest datastructures available in spark, it doesnot have any structure or schema. It is like a combination of objects, 
those objs can be stored in a list and that list can be distributed among number of machines. we can not apply any type of 
optimization as well, it is very slow


then we have dataframes which actually solved many problems which we are facing using RDDs, DF has tabular structure, has schema, its easy to use, 
almost all libs are compatible to work with dataframe, very popular almost all data professionals uses it


Then why do we have dataset ?
Dataset is like a dataframe but with some more featres, basically it is connecting 2 worlds, RDDs & dataframes plus some other features like typesafe and functional program



then why we donot use dataset and uses dataframes then ? Its because dataset are not comatible with python   (pyspark) it is only compatible with scala (spark-scala)
 & dataframes are very easy to use, you can use sql like apis, if you also workk with pandas then you need to have same language like python 
to work with in pyspark and python-pandas
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

What is Query Optimization, How spark optimizes the query ?

In spark when you write a query using sparkSQL, so before going directly to cluster for processing, spark first creats LOGICAL PLAN, what's in logical plan ?

- lets say in you code you have mentioned number of transformations like select, where, filter, joins, windows and all. In this logical plan 
it first creates order of each transformations like it can use joins first or window function first, it will smartly just decides the order on
 which order should this query needs to be run so that it can be optimized, it all will be done using CATALYST OPTIMIZER. Thorigh this obviously 
our query will be efficient and runs faster.

- Once the logical plan is ready then spark creates a PHYSICAL PLAN, In here it compares the cost of each transformations against cost model and 
select least expensive transformation from here, so for .e.g. you ave to perform join in you query in physical plan it compares join's cost plan, 
in joins we have so many types of joins like sort merge join, shuffle join, hash join, etc.. so it picks up most inexpensive one.

- Once it decides the best plan, the least expensive one then this physical plan converts inot RDDs, now theses RDDs will be goiung to diver worker 
nodes distributedly so that it can executes in parallel.


--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

<b> What is Spark Session ? </b>

spark session is the newer entry point for the spark (after spark 2.0), before spark 2.0 we were using 3 types of spark entry points 1. spark context 2. Sql Context 3. Hive context

But after spark session is been launched after spark 2.0 there is no need to manage each contexts seperately, spark session takes the responsibility and now new entry point after spark 2.0. 

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

When to use Cache() and persist()?

when we need to use the intermediate results multiple times then we can just store the result (dataframe) in memory or disk. 

What is the difference b/w Cache() and persist()?

df.persist(StorageLevel.MEMORY_AND_DISK) ---->> It beomes CACHE()
intead of writing this again & again, spark has made a dedicated function for it which is :
df.cache() # it is actually same

We rarely use others:
df.persist(StorageLevel.MEMORY_ONLY)
df.persist(StorageLevel.DISK_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK_SERIALIZED) # first try to save the data to memory, then to disk

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

Lazy Evaluation in Spark:

pyspark lazyly evaluate your code, lets say you have dataframe and you have performed multiple transformations like filter, groupby,sort , what would you think it will run right away once you submit the job ? No. It stores all the information and creates a Logical Execution Plan for all the tranformations. It will exectue it when you will trigger ACTION.

- So we have some ACTION operations and when we use that than only it execute than and before that it will be stored in logical plan thats it

- What are ACTION, df.show(), df.collect() all are action
- If you have hundreds of transformations in spark, if you dont trigger action no transformation will be executed.

