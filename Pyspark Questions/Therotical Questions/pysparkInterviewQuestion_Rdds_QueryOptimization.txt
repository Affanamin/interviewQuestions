
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

RDDs are the lowest datastructures available in spark, it doesnot have any structure or schema. It is like a combination of objects, 
those objs can be stored in a list and that list can be distributed among number of machines. we can not apply any type of 
optimization as well, it is very slow


then we have dataframes which actually solved many problems which we are facing using RDDs, DF has tabular structure, has schema, its easy to use, 
almost all libs are compatible to work with dataframe, very popular almost all data professionals uses it


Then why do we have dataset ?
Dataset is like a dataframe but with some more featres, basically it is connecting 2 worlds, RDDs & dataframes plus some other features like typesafe and functional program



then why we donot use dataset and uses dataframes then ? Its because dataset are not comatible with python   (pyspark) it is only compatible with scala (spark-scala)
 & dataframes are very easy to use, you can use sql like apis, if you also workk with pandas then you need to have same language like python 
to work with in pyspark and python-pandas
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

What is Query Optimization, How spark optimizes the query ?

In spark when you write a query using sparkSQL, so before going directly to cluster for processing, spark first creats LOGICAL PLAN, what's in logical plan ?

- lets say in you code you have mentioned number of transformations like select, where, filter, joins, windows and all. In this logical plan 
it first creates order of each transformations like it can use joins first or window function first, it will smartly just decides the order on
 which order should this query needs to be run so that it can be optimized, it all will be done using CATALYST OPTIMIZER. Thorigh this obviously 
our query will be efficient and runs faster.

- Once the logical plan is ready then spark creates a PHYSICAL PLAN, In here it compares the cost of each transformations against cost model and 
select least expensive transformation from here, so for .e.g. you ave to perform join in you query in physical plan it compares join's cost plan, 
in joins we have so many types of joins like sort merge join, shuffle join, hash join, etc.. so it picks up most inexpensive one.

- Once it decides the best plan, the least expensive one then this physical plan converts inot RDDs, now theses RDDs will be goiung to diver worker 
nodes distributedly so that it can executes in parallel.


--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

<b> What is Spark Session ? </b>

spark session is the newer entry point for the spark (after spark 2.0), before spark 2.0 we were using 3 types of spark entry points 1. spark context 2. Sql Context 3. Hive context

But after spark session is been launched after spark 2.0 there is no need to manage each contexts seperately, spark session takes the responsibility and now new entry point after spark 2.0. 

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

When to use Cache() and persist()?

when we need to use the intermediate results multiple times then we can just store the result (dataframe) in memory or disk. 

What is the difference b/w Cache() and persist()?

df.persist(StorageLevel.MEMORY_AND_DISK) ---->> It beomes CACHE()
intead of writing this again & again, spark has made a dedicated function for it which is :
df.cache() # it is actually same

We rarely use others:
df.persist(StorageLevel.MEMORY_ONLY)
df.persist(StorageLevel.DISK_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK_SERIALIZED) # first try to save the data to memory, then to disk

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

Lazy Evaluation in Spark:

pyspark lazyly evaluate your code, lets say you have dataframe and you have performed multiple transformations like filter, groupby,sort , what would you think it will run right away once you submit the job ? No. It stores all the information and creates a Logical Execution Plan for all the tranformations. It will exectue it when you will trigger ACTION.

- So we have some ACTION operations and when we use that than only it execute than and before that it will be stored in logical plan thats it

- What are ACTION, df.show(), df.collect() all are action
- If you have hundreds of transformations in spark, if you dont trigger action no transformation will be executed.

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

What are the advantages of Delta lake over traditional file formats ?

- ACID transactions (Atomacity, Consistency, Isolation, Durability) Using delta log
- Schema Enforcement and schema evolution: By default spark applies schema on read, while pyspark applies schema on write and if there is some change in schema it will thorugh an error, But we can evolve that things as well on some cases
- Optimization techniques

so these are advantages of every fileformat and on parquet file format as well.

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------

Driver OOM and Executor OOM:

Driver OOM occurs when te driver node does not have enogh memory to handle the data set, for e.g if a user runs .collect() on a data set which has greater memory than driver memory

Executor OOM can be occured due to so many reasons, but one reason is most common which is when your data is highly skewed and dataset is greater than executor memory which is performing any transformation than it can appen, How to mitigate this issue ? you can always increase the driver of executor memeory and if you have reached to the limit of executor memory than there are some optiomization techniques like AQE (Adaptive Query Execution) and salting.

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------


Q: AQE (Adaptive Query Execution):
It is a spark optimization technique which optimizes the query execution at runtime, It has 3 major powerful features:

1. Dynamic Partition Pruning: So for e.g you are processing the data and there are many many small partitions (without AQE), with this Dynamic Partition Pruning in AQE, it will coalesce all partitions and decrease the number of partitions, create 3 big partitions instead of 10 small partitions and save us from Small file problem as well which can hault the processing.

2. Join Strategies Optimizations: So for e.g you are processing the data and you have applied join on 2 big data frames, but then you have also applied filters on them too, so this feature will check and if there is a possibility of run broadcast join instead of shuffle join or anuy other costly join it will run broadcast join automatically

3. Dynamically Optimizes Skewness: 
So for e.g you are processing the data and you have one big partition and others are small one then with feature of AQE will take care of this and break down that big partition in smaller ones so that parallelism can be aceived more accurately.

How to enable AQE in spark ? It is auto enabled now after spark 3.2+

--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------




