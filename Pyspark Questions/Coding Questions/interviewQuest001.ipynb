{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058dd8bf-2410-440c-9ce2-2acd42b689ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYSPARK INTERVIEW QUESTIONS - ANSH LAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1132ee8-96e6-4954-9cd7-9f97ebaeccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c874b3f0-63db-49ee-95e6-a781fc693eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Replace this with your actual Python path\n",
    "python_path = sys.executable  # This is safe — it auto-detects your current Python path\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = python_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228c30f8-3343-4271-b723-c7935df45605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\jupyter_notebooks\\\\pythonVirtualEnv\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae1792f-93cc-44b5-8530-1407f74d00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.conf import SparkConf \n",
    "from pyspark.sql import SparkSession, HiveContext,DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StringType, StructField, StringType,LongType,DecimalType,DateType,TimestampType, IntegerType,DoubleType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee77d677-05ec-4b7b-83a6-cfbf6f2bbced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                        .appName('example-pyspark-read-and-write-from-hive') \\\n",
    "                        .master(\"local[*]\") \\\n",
    "                        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,com.crealytics:spark-excel_2.12:3.3.3_0.20.3\") \\\n",
    "                        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\\\n",
    "                        .enableHiveSupport() \\\n",
    "                        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "#                         .config(\"hive.metastore.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "#                         .config(\"hive.metastore.uris\", \"thrift://192.168.66.104:9083\") \\\n",
    "#                         .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://192.168.66.101:9000\") \\\n",
    "#                         .config(\"spark.executor.extraLibraryPath\", \"/usr/lib/x86_64-linux-gnu/\") \\\n",
    "#                         .config(\"spark.driver.extraLibraryPath\", \"/usr/lib/x86_64-linux-gnu/\") \\\n",
    "#                         .config(\"spark.driver.extraClassPath\", \"/usr/lib/x86_64-linux-gnu/jni/\") \\\n",
    "#                         .config(\"spark.executor.extraClassPath\", \"/usr/lib/x86_64-linux-gnu/jni/\") \\\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9d76d0-240c-4bd7-94dd-9cb9beb4413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and \n",
    "### retain only the latest entry based on a timestamp column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcf6627d-5b33-47bd-bb2f-ab1ea9740e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"101\", \"2023-12-01\", 100), (\"101\", \"2023-12-02\", 150), \n",
    "        (\"102\", \"2023-12-01\", 200), (\"102\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3aeca-1111-4aee-becf-60385e9af99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897dce1b-c600-42bb-8d6f-9bde3acd3e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|date      |sales|\n",
      "+----------+----------+-----+\n",
      "|101       |2023-12-01|100  |\n",
      "|101       |2023-12-02|150  |\n",
      "|102       |2023-12-01|200  |\n",
      "|102       |2023-12-02|250  |\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95d6599f-f899-4da1-807b-3905850379b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting date column from string to datestamp\n",
    "df = df.withColumn('date',f.col('date').cast(DateType()))\n",
    "# then sort the data to desc and then drop duplicates with save first and remove last\n",
    "\n",
    "\n",
    "#df = df.orderBy(f.col('product_id'),f.col('date'), ascending=[1,0]).dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034b8af-2167-40aa-bd06-04d1f0bc5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropDuplicates([\"product_id\"]) keeps the first occurrence of each product_id based on the current row order.\n",
    "\n",
    "## So, by sorting before it, you control which row gets kept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32c1ca65-6a9c-46f6-bd72-51882288ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy(f.col('product_id'),f.col('date'), ascending=[1,0]).dropDuplicates(subset=['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4de884dc-4ea3-489d-9bdd-ddf413ef84b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|       101|2023-12-02|  150|\n",
      "|       102|2023-12-02|  250|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9bec0-86a0-43bb-a96a-0e1fe84c2e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e144fe-b54a-4bca-93c9-2e689e9c6432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "623e6005-b199-47c0-935b-38879b137c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66395b86-56f7-4980-a563-f4d87e8cf754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## While processing data from multiple files with inconsistent schemas, you need to merge them into a single DataFrame. \n",
    "\n",
    "## How would you handle this inconsistency in PySpark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f843ac-bb6a-4f6c-9802-eb487bb991be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss\n",
    "df = spark.read.format('parquet')\\\n",
    "                .option('mergeSchema',True)\\\n",
    "                .load('hdfs://yourlocation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3fdb1-5a88-4f8d-bb93-8782efae8fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a149106-e6be-4166-836e-f28a2fed2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 03\n",
    "\n",
    "## 4. You are working with a real-time data pipeline, and you notice missing values in your streaming data Column - Category. How would you handle \n",
    "## null or missing values in such a scenario?\n",
    "\n",
    "## df_stream = spark.readStream.schema(\"id INT, value STRING\").csv(\"path/to/stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4cbd5-ced0-4b89-96db-d947c5b76d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a706252-03e3-4844-ac3c-d70b07511ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss\n",
    "df = df.fillNa({'Category':'N/A'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f1115db-8d45-489d-bf5d-37cdf82534f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. You need to calculate the total number of actions performed by users in a system. How would you calculate the top 5 most active users \n",
    "## based on this information?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "00ddacc5-c377-4636-9ae9-c9d94cb17a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|actions|\n",
      "+-------+-------+\n",
      "|  user1|      5|\n",
      "|  user2|      8|\n",
      "|  user3|      2|\n",
      "|  user4|     10|\n",
      "|  user2|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"user1\", 5), (\"user2\", 8), (\"user3\", 2), (\"user4\", 10), (\"user2\", 3)]\n",
    "columns = [\"user_id\", \"actions\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbbd4d25-429a-46e9-b11e-41900d87882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(f.col('user_id')).agg(sum('actions').alias(\"totalActions\")).orderBy(f.col(\"totalActions\"),ascending=False).limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bad220b1-a45c-4079-9ba6-7815d692e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.orderBy(f.col(\"totalActions\"),descending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6994e827-7b6e-42ff-a873-958c821af756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|user_id|totalActions|\n",
      "+-------+------------+\n",
      "|  user2|          11|\n",
      "|  user4|          10|\n",
      "|  user1|           5|\n",
      "|  user3|           2|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abdbec3-f2e5-4b4b-8a76-9b0f86aec8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369ae62-3e61-40f2-93ec-0f6f00108ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. While processing sales transaction data, you need to identify the most recent transaction for each customer. How would you approach this task?\n",
    "\n",
    "## Hint: Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a56f1a1b-69bb-47a5-814b-944039c9721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"cust1\", \"2023-12-01\", 100), (\"cust2\", \"2023-12-02\", 150),\n",
    "        (\"cust1\", \"2023-12-03\", 200), (\"cust2\", \"2023-12-04\", 250)]\n",
    "columns = [\"customer_id\", \"transaction_date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc7adb3d-546d-4dc9-bb3f-b0b64848ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----+\n",
      "|customer_id|transaction_date|sales|\n",
      "+-----------+----------------+-----+\n",
      "|      cust1|      2023-12-01|  100|\n",
      "|      cust2|      2023-12-02|  150|\n",
      "|      cust1|      2023-12-03|  200|\n",
      "|      cust2|      2023-12-04|  250|\n",
      "+-----------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "127919be-fc39-474d-a0c1-cb037428a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast transaction_date to datestamp from string\n",
    "\n",
    "df = df.withColumn('transaction_date', f.col('transaction_date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9c21ac2-087d-4e74-bae1-03dc8f23f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----+\n",
      "|customer_id|transaction_date|sales|\n",
      "+-----------+----------------+-----+\n",
      "|cust1      |2023-12-03      |200  |\n",
      "|cust2      |2023-12-04      |250  |\n",
      "+-----------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('flag',dense_rank().over(Window.partitionBy('customer_id').orderBy(f.col('transaction_date').desc()))) \\\n",
    "        .filter(f.col('flag') == 1) \\\n",
    "        .select(\"customer_id\",\"transaction_date\",\"sales\") \\\n",
    "        .show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b55d483b-052e-4495-9b61-ca8b4845cc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793b78d-79f0-4ca7-8633-90a6b1c72ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. You need to identify customers who haven’t made any purchases in the last 30 days. How would you filter such customers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "66621bf1-93f8-44b3-8fee-6b1d15ffda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"cust1\", \"2025-04-01\"), (\"cust2\", \"2024-11-20\"), (\"cust3\", \"2024-03-25\")]\n",
    "columns = [\"customer_id\", \"last_purchase_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "461a182f-c4ff-4a55-a969-f221b0248301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|last_purchase_date|\n",
      "+-----------+------------------+\n",
      "|      cust1|        2025-04-01|\n",
      "|      cust2|        2024-11-20|\n",
      "|      cust3|        2024-03-25|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "99ad692e-399c-4fd0-8ed1-2d7c3038145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- last_purchase_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # itis string so I need to convert it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a49baf58-73d6-40da-961a-6e5cdee86784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"last_purchase_date\", f.col(\"last_purchase_date\").cast(DateType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1ad9814c-5647-4023-bf22-c1e7cf7369a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In between of this data and -30 days from todays date\n",
    "\n",
    "#df.withColumn('gap', datediff(current_date(),'last_purchase_date')).filter(f.col('gap')>30).show()\n",
    "\n",
    "\n",
    "df  = df.withColumn(\"gap\",f.datediff(current_date(), f.col('last_purchase_date'))).filter(f.col('gap') > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8532ac8a-b97a-4102-83be-dca848360fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+---+\n",
      "|customer_id|last_purchase_date|gap|\n",
      "+-----------+------------------+---+\n",
      "|      cust2|        2024-11-20|140|\n",
      "|      cust3|        2024-03-25|380|\n",
      "+-----------+------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea95528-0db3-451e-ade0-87103c310384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
