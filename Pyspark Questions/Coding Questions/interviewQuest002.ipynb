{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410ac766-ef04-41bb-89aa-93b54a3a5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYSPARK INTERVIEW QUESTIONS - ANSH LAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a641023-7c43-44d0-8ba7-87d36049a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53b75b6-4802-4190-9822-c574128cc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Replace this with your actual Python path\n",
    "python_path = sys.executable  # This is safe â€” it auto-detects your current Python path\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = python_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db48486c-0bad-4f28-a8bb-6b4daef6e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.conf import SparkConf \n",
    "from pyspark.sql import SparkSession, HiveContext,DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StringType, StructField, StringType,LongType,DecimalType,DateType,TimestampType, IntegerType,DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218c5163-e39f-4242-8d97-efdb47462c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                        .appName('example-pyspark-read-and-write-from-hive') \\\n",
    "                        .master(\"local[*]\") \\\n",
    "                        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,com.crealytics:spark-excel_2.12:3.3.3_0.20.3\") \\\n",
    "                        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\\\n",
    "                        .enableHiveSupport() \\\n",
    "                        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d59228-242e-4834-9c55-b8d3a1b66b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7afbb6ae-7f5e-4315-a1e9-58a728d5dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"customer1\", \"The product is great\"), (\"customer2\", \"Great product, fast delivery\"), (\"customer3\", \"Not bad, could be better\")]\n",
    "columns = [\"customer_id\", \"feedback\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc082252-7bc3-4fd0-85ca-591ac51cc8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------+\n",
      "|customer_id|feedback                    |\n",
      "+-----------+----------------------------+\n",
      "|customer1  |The product is great        |\n",
      "|customer2  |Great product, fast delivery|\n",
      "|customer3  |Not bad, could be better    |\n",
      "+-----------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d76f71-a3af-4626-8125-43597b6bfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn('feedback', split(f.col('feedback'),' '))\n",
    "# df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc3ca3c9-1cf2-43c8-8c80-d72569f63f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('feedback',f.explode(f.split('feedback',' ')))\\\n",
    "        .withColumn('feedback',lower('feedback'))\\\n",
    "        .groupBy('feedback').agg(count('feedback').alias('wordCount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18e81c8d-7f21-47c0-8962-684ed2466d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|feedback|wordCount|\n",
      "+--------+---------+\n",
      "|   great|        2|\n",
      "|      is|        1|\n",
      "|     the|        1|\n",
      "| product|        1|\n",
      "|    fast|        1|\n",
      "|delivery|        1|\n",
      "|product,|        1|\n",
      "|   could|        1|\n",
      "|     not|        1|\n",
      "|      be|        1|\n",
      "|    bad,|        1|\n",
      "|  better|        1|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e65939-080d-48b8-8d07-6dba12ae6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. You need to calculate the cumulative sum of sales over time for each product. How would you approach this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75612178-dd0e-4839-9dfd-c7a364775ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|  product1|2023-12-01|  100|\n",
      "|  product2|2023-12-02|  200|\n",
      "|  product1|2023-12-03|  150|\n",
      "|  product2|2023-12-04|  250|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-02\", 200),\n",
    "        (\"product1\", \"2023-12-03\", 150), (\"product2\", \"2023-12-04\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b45d9954-43a1-4a9c-8e45-b1f2d1b38c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cast date column from string to Datestamp:\n",
    "\n",
    "df = df.withColumn(\"date\", f.col('date').cast(DateType()))\n",
    "## applying window function with Sum as aggregation:::\n",
    "\n",
    "\n",
    "df = df.withColumn(\"CumalativeSum\", sum(\"sales\").over(Window.partitionBy('product_id').orderBy(f.col(\"date\")))) \\\n",
    "        .orderBy(f.col(\"date\"), ascending=True)\n",
    "\n",
    "#.orderBy(f.col(\"totalActions\"),ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cd958ce-b8f6-4945-9155-e2d35865e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-------------+\n",
      "|product_id|      date|sales|CumalativeSum|\n",
      "+----------+----------+-----+-------------+\n",
      "|  product1|2023-12-01|  100|          100|\n",
      "|  product2|2023-12-02|  200|          200|\n",
      "|  product1|2023-12-03|  150|          250|\n",
      "|  product2|2023-12-04|  250|          450|\n",
      "+----------+----------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471ae9a-f6c4-48be-8369-b9bd7d56b8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424861e-fe98-40cf-a1a8-0eb3f2dbc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### 10. While preparing a data pipeline, you notice some duplicate rows in a dataset. How would you remove the duplicates without affecting \n",
    "## the original order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e65d07a0-613c-47b1-923b-647865893355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 25|\n",
      "| Jane| 30|\n",
      "| John| 25|\n",
      "|Alice| 22|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", 25), (\"Jane\", 30), (\"John\", 25), (\"Alice\", 22)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "336b2d40-8c3b-4640-977c-78d394431455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a273585c-b5dc-4eb8-910b-4f779fb9ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 22|\n",
      "| Jane| 30|\n",
      "| John| 25|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Resolving it with rownumber() window function, when we are getting 2 just filter it out\n",
    "\n",
    "df = df.withColumn('Flag', row_number().over(Window.partitionBy(\"name\").orderBy(f.col(\"age\").asc()))) \\\n",
    "        .filter(f.col(\"flag\") == 1)\\\n",
    "        .select(\"name\", \"age\")\n",
    "        \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e280746-526d-485a-92ac-a7046fdc92a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb471b2-6c07-4dc8-b4ee-cf9850943f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. You are working with user activity data and need to calculate the average session duration per user. How would you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "063d2f20-d7a9-493d-97c7-fde7d86d26f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+\n",
      "|user_id|session_date|duration|\n",
      "+-------+------------+--------+\n",
      "|  user1|  2023-12-01|      50|\n",
      "|  user1|  2023-12-02|      60|\n",
      "|  user2|  2023-12-01|      45|\n",
      "|  user2|  2023-12-03|      75|\n",
      "+-------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"user1\", \"2023-12-01\", 50), (\"user1\", \"2023-12-02\", 60), \n",
    "        (\"user2\", \"2023-12-01\", 45), (\"user2\", \"2023-12-03\", 75)]\n",
    "columns = [\"user_id\", \"session_date\", \"duration\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f1e6fb1-54e0-40f7-b2fe-5d7a495d626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|user_id|avgDuaration|\n",
      "+-------+------------+\n",
      "|  user1|        55.0|\n",
      "|  user2|        60.0|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## avg:::\n",
    "\n",
    "df=df.groupBy('user_id').agg(avg('duration').alias(\"avgDuaration\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa2ba6-f91d-4abf-aa6a-b837134b34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12. While analyzing sales data, you need to find the product with the highest sales for each month. How would you accomplish this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afbc2e-7e46-461d-9b5b-3dbae73a2051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f8572c17-fbb7-4649-904a-932af6643b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-01\", 150), \n",
    "        (\"product1\", \"2023-12-02\", 200), (\"product2\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4aa3555c-9fc2-4230-bb5b-44a81bd0b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b6cb810-b8cc-460f-90aa-045d487d2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"date\", f.col(\"date\").cast(DateType()))\\\n",
    "        .withColumn(\"month\",month(\"date\").alias('month'))\n",
    "\n",
    "## Best Product FOr each Month --> anserw will be product2\n",
    "## select product_id, month, sum(sales) from table group by product_id, month order by sum(sales) desc;\n",
    "## ok great, this will be working like that exactly, then we also need to add window function (any ranking function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95ec46ce-a862-4d80-baa3-ca2b3f0826f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+\n",
      "|product_id|      date|sales|month|\n",
      "+----------+----------+-----+-----+\n",
      "|  product1|2023-12-01|  100|   12|\n",
      "|  product2|2023-12-01|  150|   12|\n",
      "|  product1|2023-12-02|  200|   12|\n",
      "|  product2|2023-12-02|  250|   12|\n",
      "+----------+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7aaf9caa-8cda-40a3-8e9c-07073144d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(f.col(\"product_id\"),f.col(\"month\")).agg(sum('sales').alias(\"sumSales\"))\\\n",
    "        .orderBy(f.col('sumSales'), ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ca1ac31-5214-4b5b-8c3c-641e3508f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|product_id|month|sumSales|\n",
      "+----------+-----+--------+\n",
      "|  product2|   12|     400|\n",
      "|  product1|   12|     300|\n",
      "+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bafdbf01-bd47-4c58-b413-953533133d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will be aplying window function here Desnse rank   ###.desc with windowFunction --->>> alwaysss .....\n",
    "\n",
    "df = df.withColumn(\"ranking\",dense_rank().over(Window.partitionBy('month').orderBy(f.col(\"sumSales\").desc())))\\\n",
    "        .filter(f.col(\"ranking\") == 1)\\\n",
    "        .select(\"product_id\",\"month\",\"sumSales\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e222a0b6-5ee5-45c5-8116-5ca3a2b22f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|product_id|month|sumSales|\n",
      "+----------+-----+--------+\n",
      "|  product2|   12|     400|\n",
      "+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d98e23-3fc4-4789-8fba-a139a7ef64d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87e5c9-e6d5-4caa-a0d3-c2c5a6f558e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 13. You are working with a large Delta table that is frequently updated by multiple users. The data is stored in partitions, \n",
    "### and sometimes updates can cause inconsistent reads due to concurrent transactions. How would you ensure ACID compliance and \n",
    "### avoid data corruption in PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39c8b2-0acf-4caf-9a6b-2df7c95494f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACID properties can be assured using delta log, which is being created predefined when you create a delta table\n",
    "### For Data corruption we will be using following upserting syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "060ed0e5-621f-4e8f-b1a1-a4cd3aef15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading new data which is in parqut format\n",
    "df = spark.read.format('parquet').load('path')\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "## Reading data from delta table::\n",
    "\n",
    "deltaTbl = DeltaTable.forPath(spark, \"/path/to/delta/table\")\n",
    "\n",
    "deltaTbl.alias('trg').merge(df.alias('src'),\"src.id == trg.id\")\\\n",
    "                    .whenNotMatchedInsertAll()\\\n",
    "                    .whenMatchedUpdateAll()\\\n",
    "                    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c9b54-5ea4-4b91-8755-7502829b8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14. You need to process a large dataset stored in PARQUET format and ensure that all columns have the right schema (Almost). How would you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac03fed-4253-4f4f-8ab8-f25a8a61fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet')\\\n",
    "                .opion(\"inferSchema\", True)\\  ## this will infer schema and you not need to get it manually\n",
    "                .load(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46a7a7-6122-4412-b378-d3c28cf145d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 15. You are reading a CSV file and need to handle corrupt records gracefully by skipping them. How would you configure this in PySpark?\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "                .opion(\"mode\", \"DROPMALFORMED\")\\  ## this will drop all malformed records\n",
    "                .load(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7c004-aad8-46ba-a135-42737f4ce1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9d780-b74d-4858-8827-dd66f9acd66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705dfec-e034-4db9-b6c6-e19523e2776e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
