{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410ac766-ef04-41bb-89aa-93b54a3a5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYSPARK INTERVIEW QUESTIONS - ANSH LAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a641023-7c43-44d0-8ba7-87d36049a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53b75b6-4802-4190-9822-c574128cc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Replace this with your actual Python path\n",
    "python_path = sys.executable  # This is safe â€” it auto-detects your current Python path\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = python_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db48486c-0bad-4f28-a8bb-6b4daef6e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.conf import SparkConf \n",
    "from pyspark.sql import SparkSession, HiveContext,DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StringType, StructField, StringType,LongType,DecimalType,DateType,TimestampType, IntegerType,DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218c5163-e39f-4242-8d97-efdb47462c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                        .appName('example-pyspark-read-and-write-from-hive') \\\n",
    "                        .master(\"local[*]\") \\\n",
    "                        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,com.crealytics:spark-excel_2.12:3.3.3_0.20.3\") \\\n",
    "                        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\\\n",
    "                        .enableHiveSupport() \\\n",
    "                        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74aa3f59-6744-4960-be60-37ef177a7f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msda\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sda' is not defined"
     ]
    }
   ],
   "source": [
    "sda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "46d59228-242e-4834-9c55-b8d3a1b66b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7afbb6ae-7f5e-4315-a1e9-58a728d5dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"customer1\", \"The product is great\"), (\"customer2\", \"Great product, fast delivery\"), (\"customer3\", \"Not bad, could be better\")]\n",
    "columns = [\"customer_id\", \"feedback\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc082252-7bc3-4fd0-85ca-591ac51cc8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------+\n",
      "|customer_id|feedback                    |\n",
      "+-----------+----------------------------+\n",
      "|customer1  |The product is great        |\n",
      "|customer2  |Great product, fast delivery|\n",
      "|customer3  |Not bad, could be better    |\n",
      "+-----------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31d76f71-a3af-4626-8125-43597b6bfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn('feedback', split(f.col('feedback'),' '))\n",
    "# df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc3ca3c9-1cf2-43c8-8c80-d72569f63f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('feedback',f.explode(f.split('feedback',' ')))\\\n",
    "        .withColumn('feedback',lower('feedback'))\\\n",
    "        .groupBy('feedback').agg(count('feedback').alias('wordCount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "18e81c8d-7f21-47c0-8962-684ed2466d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|feedback|wordCount|\n",
      "+--------+---------+\n",
      "|   great|        2|\n",
      "|      is|        1|\n",
      "|     the|        1|\n",
      "| product|        1|\n",
      "|    fast|        1|\n",
      "|delivery|        1|\n",
      "|product,|        1|\n",
      "|   could|        1|\n",
      "|     not|        1|\n",
      "|      be|        1|\n",
      "|    bad,|        1|\n",
      "|  better|        1|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81e65939-080d-48b8-8d07-6dba12ae6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. You need to calculate the cumulative sum of sales over time for each product. How would you approach this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75612178-dd0e-4839-9dfd-c7a364775ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|  product1|2023-12-01|  100|\n",
      "|  product2|2023-12-02|  200|\n",
      "|  product1|2023-12-03|  150|\n",
      "|  product2|2023-12-04|  250|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-02\", 200),\n",
    "        (\"product1\", \"2023-12-03\", 150), (\"product2\", \"2023-12-04\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b45d9954-43a1-4a9c-8e45-b1f2d1b38c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cast date column from string to Datestamp:\n",
    "\n",
    "df = df.withColumn(\"date\", f.col('date').cast(DateType()))\n",
    "## applying window function with Sum as aggregation:::\n",
    "\n",
    "\n",
    "df = df.withColumn(\"CumalativeSum\", sum(\"sales\").over(Window.partitionBy('product_id').orderBy(f.col(\"date\")))) \\\n",
    "        .orderBy(f.col(\"date\"), ascending=True)\n",
    "\n",
    "#.orderBy(f.col(\"totalActions\"),ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cd958ce-b8f6-4945-9155-e2d35865e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-------------+\n",
      "|product_id|      date|sales|CumalativeSum|\n",
      "+----------+----------+-----+-------------+\n",
      "|  product1|2023-12-01|  100|          100|\n",
      "|  product2|2023-12-02|  200|          200|\n",
      "|  product1|2023-12-03|  150|          250|\n",
      "|  product2|2023-12-04|  250|          450|\n",
      "+----------+----------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471ae9a-f6c4-48be-8369-b9bd7d56b8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5424861e-fe98-40cf-a1a8-0eb3f2dbc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### 10. While preparing a data pipeline, you notice some duplicate rows in a dataset. How would you remove the duplicates without affecting \n",
    "## the original order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e65d07a0-613c-47b1-923b-647865893355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 25|\n",
      "| Jane| 30|\n",
      "| John| 25|\n",
      "|Alice| 22|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", 25), (\"Jane\", 30), (\"John\", 25), (\"Alice\", 22)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "336b2d40-8c3b-4640-977c-78d394431455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a273585c-b5dc-4eb8-910b-4f779fb9ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 22|\n",
      "| Jane| 30|\n",
      "| John| 25|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Resolving it with rownumber() window function, when we are getting 2 just filter it out\n",
    "\n",
    "df = df.withColumn('Flag', row_number().over(Window.partitionBy(\"name\").orderBy(f.col(\"age\").asc()))) \\\n",
    "        .filter(f.col(\"flag\") == 1)\\\n",
    "        .select(\"name\", \"age\")\n",
    "        \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e280746-526d-485a-92ac-a7046fdc92a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ddb471b2-6c07-4dc8-b4ee-cf9850943f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. You are working with user activity data and need to calculate the average session duration per user. How would you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "063d2f20-d7a9-493d-97c7-fde7d86d26f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+\n",
      "|user_id|session_date|duration|\n",
      "+-------+------------+--------+\n",
      "|  user1|  2023-12-01|      50|\n",
      "|  user1|  2023-12-02|      60|\n",
      "|  user2|  2023-12-01|      45|\n",
      "|  user2|  2023-12-03|      75|\n",
      "+-------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"user1\", \"2023-12-01\", 50), (\"user1\", \"2023-12-02\", 60), \n",
    "        (\"user2\", \"2023-12-01\", 45), (\"user2\", \"2023-12-03\", 75)]\n",
    "columns = [\"user_id\", \"session_date\", \"duration\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7f1e6fb1-54e0-40f7-b2fe-5d7a495d626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|user_id|avgDuaration|\n",
      "+-------+------------+\n",
      "|  user1|        55.0|\n",
      "|  user2|        60.0|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## avg:::\n",
    "\n",
    "df=df.groupBy('user_id').agg(avg('duration').alias(\"avgDuaration\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2caa2ba6-f91d-4abf-aa6a-b837134b34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12. While analyzing sales data, you need to find the product with the highest sales for each month. How would you accomplish this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afbc2e-7e46-461d-9b5b-3dbae73a2051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f8572c17-fbb7-4649-904a-932af6643b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-01\", 150), \n",
    "        (\"product1\", \"2023-12-02\", 200), (\"product2\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4aa3555c-9fc2-4230-bb5b-44a81bd0b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b6cb810-b8cc-460f-90aa-045d487d2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"date\", f.col(\"date\").cast(DateType()))\\\n",
    "        .withColumn(\"month\",month(\"date\").alias('month'))\n",
    "\n",
    "## Best Product FOr each Month --> anserw will be product2\n",
    "## select product_id, month, sum(sales) from table group by product_id, month order by sum(sales) desc;\n",
    "## ok great, this will be working like that exactly, then we also need to add window function (any ranking function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95ec46ce-a862-4d80-baa3-ca2b3f0826f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+\n",
      "|product_id|      date|sales|month|\n",
      "+----------+----------+-----+-----+\n",
      "|  product1|2023-12-01|  100|   12|\n",
      "|  product2|2023-12-01|  150|   12|\n",
      "|  product1|2023-12-02|  200|   12|\n",
      "|  product2|2023-12-02|  250|   12|\n",
      "+----------+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7aaf9caa-8cda-40a3-8e9c-07073144d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(f.col(\"product_id\"),f.col(\"month\")).agg(sum('sales').alias(\"sumSales\"))\\\n",
    "        .orderBy(f.col('sumSales'), ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ca1ac31-5214-4b5b-8c3c-641e3508f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|product_id|month|sumSales|\n",
      "+----------+-----+--------+\n",
      "|  product2|   12|     400|\n",
      "|  product1|   12|     300|\n",
      "+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bafdbf01-bd47-4c58-b413-953533133d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will be aplying window function here Desnse rank   ###.desc with windowFunction --->>> alwaysss .....\n",
    "\n",
    "df = df.withColumn(\"ranking\",dense_rank().over(Window.partitionBy('month').orderBy(f.col(\"sumSales\").desc())))\\\n",
    "        .filter(f.col(\"ranking\") == 1)\\\n",
    "        .select(\"product_id\",\"month\",\"sumSales\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e222a0b6-5ee5-45c5-8116-5ca3a2b22f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|product_id|month|sumSales|\n",
      "+----------+-----+--------+\n",
      "|  product2|   12|     400|\n",
      "+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d98e23-3fc4-4789-8fba-a139a7ef64d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4e87e5c9-e6d5-4caa-a0d3-c2c5a6f558e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 13. You are working with a large Delta table that is frequently updated by multiple users. The data is stored in partitions, \n",
    "### and sometimes updates can cause inconsistent reads due to concurrent transactions. How would you ensure ACID compliance and \n",
    "### avoid data corruption in PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0f39c8b2-0acf-4caf-9a6b-2df7c95494f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACID properties can be assured using delta log, which is being created predefined when you create a delta table\n",
    "### For Data corruption we will be using following upserting syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "060ed0e5-621f-4e8f-b1a1-a4cd3aef15b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/D:/jupyter_notebooks/path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Reading new data which is in parqut format\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m## Reading data from delta table::\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/D:/jupyter_notebooks/path."
     ]
    }
   ],
   "source": [
    "## Reading new data which is in parqut format\n",
    "df = spark.read.format('parquet').load('path')\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "## Reading data from delta table::\n",
    "\n",
    "deltaTbl = DeltaTable.forPath(spark, \"/path/to/delta/table\")\n",
    "\n",
    "deltaTbl.alias('trg').merge(df.alias('src'),\"src.id == trg.id\")\\\n",
    "                    .whenNotMatchedInsertAll()\\\n",
    "                    .whenMatchedUpdateAll()\\\n",
    "                    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "323c9b54-5ea4-4b91-8755-7502829b8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14. You need to process a large dataset stored in PARQUET format and ensure that all columns have the right schema (Almost). How would you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac03fed-4253-4f4f-8ab8-f25a8a61fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet')\\\n",
    "                .opion(\"inferSchema\", True)\\  ## this will infer schema and you not need to get it manually\n",
    "                .load(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46a7a7-6122-4412-b378-d3c28cf145d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 15. You are reading a CSV file and need to handle corrupt records gracefully by skipping them. How would you configure this in PySpark?\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "                .opion(\"mode\", \"DROPMALFORMED\")\\  ## this will drop all malformed records\n",
    "                .load(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7c004-aad8-46ba-a135-42737f4ce1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9d780-b74d-4858-8827-dd66f9acd66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You have a dataset containing the names of employees and their departments. You need to find the department with the most employees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1705dfec-e034-4db9-b6c6-e19523e2776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|employee_name| department|\n",
      "+-------------+-----------+\n",
      "|        Alice|         HR|\n",
      "|          Bob|    Finance|\n",
      "|      Charlie|         HR|\n",
      "|        David|Engineering|\n",
      "|          Eve|    Finance|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", \"HR\"), (\"Bob\", \"Finance\"), (\"Charlie\", \"HR\"), (\"David\", \"Engineering\"), (\"Eve\", \"Finance\")]\n",
    "columns = [\"employee_name\", \"department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3b6245f6-d2b1-4eff-addd-3759cae43588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"department\").agg(count(f.col(\"employee_name\")).alias(\"CountEmployees\"))\\\n",
    "        .orderBy(f.col(\"CountEmployees\"),ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8772deff-247c-4f7f-8482-36ac350a0aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "| department|CountEmployees|\n",
      "+-----------+--------------+\n",
      "|         HR|             2|\n",
      "|    Finance|             2|\n",
      "|Engineering|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b195d7-ca1f-473a-9601-15af15f0bc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d49049-ce5b-4b62-a41f-993f79e9f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 23. While processing sales data, you need to classify each transaction as either 'High' or 'Low' based on its amount. How would you \n",
    "### achieve this using a when condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b37c617-c60c-4de1-9030-80e953dccf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|sales|\n",
      "+----------+-----+\n",
      "|  product1|  100|\n",
      "|  product2|  300|\n",
      "|  product3|   50|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 300), (\"product3\", 50)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "728227a1-4d03-442d-8181-c749f77ab690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"classify\", when(f.col(\"sales\") > 250,\"High\").otherwise(\"Low\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "591f8fa4-7e4c-42b2-85ae-3d9d934cd4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|product_id|sales|classify|\n",
      "+----------+-----+--------+\n",
      "|  product1|  100|     Low|\n",
      "|  product2|  300|    High|\n",
      "|  product3|   50|     Low|\n",
      "+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697739a-6b98-46db-95b6-6bdfa41a4fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c35a1e71-1e3a-49d3-aa51-5108d636ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 24. While analyzing a large dataset, you need to create a new column that holds a timestamp of when the record was processed. \n",
    "## How would you implement this and what can be the best USE CASE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "352928c3-3c01-41e4-9b30-ac45914db344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|sales|\n",
      "+----------+-----+\n",
      "|  product1|  100|\n",
      "|  product2|  200|\n",
      "|  product3|  300|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "382c9868-290b-4dda-ba56-2f35e4cca3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "processedAt = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "df = df.withColumn(\"processed_at\",lit(processedAt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03216d1c-6b98-41be-8dd9-ae23a0295000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------------+\n",
      "|product_id|sales|       processed_at|\n",
      "+----------+-----+-------------------+\n",
      "|  product1|  100|2025-04-10 14:30:51|\n",
      "|  product2|  200|2025-04-10 14:30:51|\n",
      "|  product3|  300|2025-04-10 14:30:51|\n",
      "+----------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e7c2917-49fe-4e33-8aba-d12d5a047726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(product_id='product1', sales=100, processed_at='2025-04-10 14:30:51'),\n",
       " Row(product_id='product2', sales=200, processed_at='2025-04-10 14:30:51'),\n",
       " Row(product_id='product3', sales=300, processed_at='2025-04-10 14:30:51')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58678f07-d486-4075-8415-ace927b2da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 31. Your company uses a large-scale data pipeline that reads from Delta tables and processes data using complex aggregations. \n",
    "### However, performance is becoming an issue due to the growing dataset size. How would you optimize the performance of the pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16d28b-c87a-4e8f-8c8e-dfe33428e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "OPTIMIZE tabledelta ZORDER BY ('order_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007df6e-8e32-4620-9ac6-b1536c870703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. what is OPTIMIZE & ZORDER doing here ?\n",
    "\n",
    "# It (OPTIMIZE) will coalesce the partitions, so it will create fewer partitions of bigger size\n",
    "# It (ZORDER) will sort the data and inseret it in partitions like buketing do, so in that case you dont need to \n",
    "# go to other partitions and data shuffling will be avoided, and if you are not going to second partition than we called as data skiping\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da515cd6-68ab-4955-9bea-4bb34cdf8d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a9c4e2b-c7e6-4344-b00c-f3cafb7eb8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 43. You are processing sales data. Group by product categories and create a list of \n",
    "### all product names in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85e1f7be-1757-4feb-940e-edbacd17620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|   category|   product|\n",
      "+-----------+----------+\n",
      "|Electronics|    Laptop|\n",
      "|Electronics|Smartphone|\n",
      "|  Furniture|     Chair|\n",
      "|  Furniture|     Table|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Electronics\", \"Laptop\"), (\"Electronics\", \"Smartphone\"), (\"Furniture\", \"Chair\"), (\"Furniture\", \"Table\")]\n",
    "columns = [\"category\", \"product\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6a22b44-b9a8-47be-8609-0697be6614a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"category\").agg(collect_list(f.col(\"product\")).alias(\"products\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "869f381f-2342-4729-8e8c-2f5f1187a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|   category|            products|\n",
      "+-----------+--------------------+\n",
      "|Electronics|[Laptop, Smartphone]|\n",
      "|  Furniture|      [Chair, Table]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662046da-fc35-48de-9423-442486269aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  You are analyzing orders. Group by customer IDs and list all unique product IDs each customer purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecd68485-1d06-4ad9-8d66-1edab73afd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|product_id|\n",
      "+-----------+----------+\n",
      "|        101|      P001|\n",
      "|        101|      P002|\n",
      "|        102|      P001|\n",
      "|        101|      P001|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(101, \"P001\"), (101, \"P002\"), (102, \"P001\"), (101, \"P001\")]\n",
    "columns = [\"customer_id\", \"product_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02d0c0ba-801a-42e3-add6-f5b2fcf9c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"customer_id\").agg(collect_set(f.col(\"product_id\")).alias(\"product_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "687909db-ed62-4103-9608-779e34452c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|  product_id|\n",
      "+-----------+------------+\n",
      "|        101|[P002, P001]|\n",
      "|        102|      [P001]|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc067de-880f-4d0a-86ab-a86dfeb97319",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 45. For customer records, combine first and last names only if the email address exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29e8005a-b2cd-4307-8ed3-b5eab4905860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|first_name|last_name|               email|\n",
      "+----------+---------+--------------------+\n",
      "|      John|      Doe|john.doe@example.com|\n",
      "|      Jane|    Smith|                NULL|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", \"Doe\", \"john.doe@example.com\"), (\"Jane\", \"Smith\", None)]\n",
    "columns = [\"first_name\", \"last_name\", \"email\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5293d7d1-a611-47a4-bd0f-603ffe12d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CombinedName\", when(f.col(\"email\").isNotNull(), f.concat(f.col(\"first_name\"), f.col(\"last_name\") ) ).otherwise(None) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db53a1bf-a8cb-46e4-b9b2-0308788bf56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------------+\n",
      "|first_name|last_name|               email|CombinedName|\n",
      "+----------+---------+--------------------+------------+\n",
      "|      John|      Doe|john.doe@example.com|     JohnDoe|\n",
      "|      Jane|    Smith|                NULL|        NULL|\n",
      "+----------+---------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d11be4a-6ab3-405d-8faa-7c2dd3aee822",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OR ###\n",
    "\n",
    "df = df.withColumn(\"CombinedName\", when(f.col(\"email\").isNotNull(), f.concat_ws(\"-\",f.col(\"first_name\"),f.col(\"last_name\")) ).otherwise(None) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4107e2a4-c271-4d6d-ba8d-58194bd0fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------------+\n",
      "|first_name|last_name|               email|CombinedName|\n",
      "+----------+---------+--------------------+------------+\n",
      "|      John|      Doe|john.doe@example.com|    John-Doe|\n",
      "|      Jane|    Smith|                NULL|        NULL|\n",
      "+----------+---------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521562e4-8846-4aef-b2ba-c74de56d58c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10349efb-7586-4d60-a755-954cd151797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 46. You have a DataFrame containing customer IDs and a list of their purchased product IDs. \n",
    "## Calculate the number of products each customer has purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b480af-002f-4510-a25d-7e4c2dcbd964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed88ed45-53f6-476b-a394-cf4a2ffe5260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|customer_id|         product_ids|\n",
      "+-----------+--------------------+\n",
      "|          1|[prod1, prod2, pr...|\n",
      "|          2|             [prod4]|\n",
      "|          3|      [prod5, prod6]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, [\"prod1\", \"prod2\", \"prod3\"]),\n",
    "    (2, [\"prod4\"]),\n",
    "    (3, [\"prod5\", \"prod6\"]),\n",
    "]\n",
    "myschema = \"customer_id INT ,product_ids array<STRING>\"\n",
    "\n",
    "df = spark.createDataFrame(data, myschema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a31a3597-c954-4db6-99d4-b05904e798a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------+\n",
      "|customer_id|         product_ids|NumberOfProds|\n",
      "+-----------+--------------------+-------------+\n",
      "|          1|[prod1, prod2, pr...|            3|\n",
      "|          2|             [prod4]|            1|\n",
      "|          3|      [prod5, prod6]|            2|\n",
      "+-----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =df.withColumn(\"NumberOfProds\",size(f.col(\"product_ids\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd45a0-767f-4675-9a1d-2f6856d94e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You have employee IDs of varying lengths. Ensure all IDs are 6 characters long by padding with leading zeroes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "393bf614-06ed-4abc-b247-2395300eb1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|        123|\n",
      "|       4567|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"1\",),\n",
    "    (\"123\",),\n",
    "    (\"4567\",),\n",
    "]\n",
    "schema = [\"employee_id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1ad7995-1757-490e-bb79-f80374d69beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|     000001|\n",
      "|     000123|\n",
      "|     004567|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =df.withColumn(\"employee_id\",f.lpad(f.col(\"employee_id\"),6,\"0\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c8419-1c67-494f-9277-b573d66585bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 48. You need to validate phone numbers by checking if they start with \"91\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbf0f2cb-1fd1-4b37-b7d6-d38f07d4a371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|phone_number|\n",
      "+------------+\n",
      "|911234567890|\n",
      "|811234567890|\n",
      "|912345678901|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"911234567890\",),\n",
    "    (\"811234567890\",),\n",
    "    (\"912345678901\",),\n",
    "]\n",
    "schema = [\"phone_number\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4d5da1d-8b67-4888-9f54-4e820e60caed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|phone_number|\n",
      "+------------+\n",
      "|911234567890|\n",
      "|912345678901|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(f.substring(f.col(\"phone_number\"),1,2) == \"91\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2050eb9d-ed3e-4cf7-afba-99ad7e4f871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **49. You have a dataset with courses taken by students. Calculate the average number of courses per student.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "071cdc2c-4e37-4baf-b7de-c08f375df027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|student_id|           courses|\n",
      "+----------+------------------+\n",
      "|         1|   [Math, Science]|\n",
      "|         2|         [History]|\n",
      "|         3|[Art, PE, Biology]|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, [\"Math\", \"Science\"]),\n",
    "    (2, [\"History\"]),\n",
    "    (3, [\"Art\", \"PE\", \"Biology\"]),\n",
    "]\n",
    "schema = [\"student_id\", \"courses\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af1510d2-e3d4-4f3d-8f9d-bc1ef8ec7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|avg(CourseTakenCountAvg)|\n",
      "+------------------------+\n",
      "|                     2.0|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"CourseTakenCountAvg\",f.size(f.col(\"courses\"))).groupBy().agg(avg(\"CourseTakenCountAvg\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2538f-10c1-466c-8aba-5353361bcabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221a753-7a39-4c2d-bd24-fae11be64029",
   "metadata": {},
   "outputs": [],
   "source": [
    "## coalesce SQL USecase:::::\n",
    "\n",
    "## 50. You have a dataset with primary and secondary contact numbers. Use the primary number if available; otherwise, use the secondary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7fef26df-0831-4e9d-90c5-01952ffd6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|primary_contact|secondary_contact|\n",
      "+---------------+-----------------+\n",
      "|           NULL|       1234567890|\n",
      "|     9876543210|             NULL|\n",
      "|     7894561230|       4567891230|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (None, \"1234567890\"),\n",
    "    (\"9876543210\", None),\n",
    "    (\"7894561230\", \"4567891230\"),\n",
    "]\n",
    "schema = [\"primary_contact\", \"secondary_contact\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0469a38f-ff20-4b13-a9b8-f15401d0d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+------------+\n",
      "|primary_contact|secondary_contact|finalContact|\n",
      "+---------------+-----------------+------------+\n",
      "|           NULL|       1234567890|  1234567890|\n",
      "|     9876543210|             NULL|  9876543210|\n",
      "|     7894561230|       4567891230|  7894561230|\n",
      "+---------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## This problem is similar to coalesce function in sql, we do have coalesce in spoark as well:::\n",
    "\n",
    "\n",
    "df = df.withColumn(\"finalContact\",f.coalesce(f.col(\"primary_contact\"),f.col(\"secondary_contact\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28782920-17f9-477a-8d37-9bd1bea1d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You are categorizing product codes based on their lengths. If the length is 5, label it as \"Standard\"; otherwise, label it as \"Custom\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f202856-074e-472d-b73b-30488e14feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|product_code|\n",
      "+------------+\n",
      "|       prod1|\n",
      "|      prd234|\n",
      "|      pr9876|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"prod1\",),\n",
    "    (\"prd234\",),\n",
    "    (\"pr9876\",),\n",
    "]\n",
    "schema = [\"product_code\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9fd589f2-b318-44de-b6d0-f9cca0b81f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"flagProdCode\", f.when(f.length(f.col(\"product_code\")) == 5,\"Standard\" ).otherwise(\"Custom\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5b5c254-d2a3-4c08-a644-d9bec6adaa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|product_code|flagProdCode|\n",
      "+------------+------------+\n",
      "|       prod1|    Standard|\n",
      "|      prd234|      Custom|\n",
      "|      pr9876|      Custom|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec6d2a-511b-4ba6-bf4e-819841547811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ce162-3e94-477d-84dc-8a91a06bca80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75726b16-2890-4f55-bd67-881e1c82ede4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
